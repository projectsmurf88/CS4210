# -------------------------------------------------------------------------
# AUTHOR: Gavin Hughes
# FILENAME: bagging_random_forest.py
# SPECIFICATION: test accuracy of single decision tree vs ensemble vs random forest
# FOR: CS 4210- Assignment #3
# TIME SPENT: 1 hour
# -------------------------------------------------------------------------

# IMPORTANT NOTE: DO NOT USE ANY ADVANCED PYTHON LIBRARY TO COMPLETE THIS CODE SUCH AS numpy OR pandas.
# You have to work here only with standard vectors and arrays

# importing some Python libraries
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
import csv

dbTraining = []
dbTest = []
X_training = []
Y_training = []
classVotes = []  # this array will be used to count the votes of each classifier

training = 'optdigits.tra'
test = 'optdigits.tes'
num_bootstraps = 20

# reading the training data from a csv file and populate dbTraining
# --> add your Python code here
with open(training, 'r') as csvfile:
    reader = csv.reader(csvfile)
    for j, row in enumerate(reader):
        if j > 0:  # skipping the header
            dbTraining.append(row)

# reading the test data from a csv file and populate dbTest
# --> add your Python code here
with open(test, 'r') as csvfile:
    reader = csv.reader(csvfile)
    for j, row in enumerate(reader):
        if j > 0:  # skipping the header
            dbTest.append(row)
            classVotes.append([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])

# initializing the class votes for each test sample. Example: classVotes.append([0,0,0,0,0,0,0,0,0,0])
# --> add your Python code here ^^^bottom of loop
single_correct = 0
ensemble_correct = 0
rf_correct = 0

print("Started my base and ensemble classifier ...")

# we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample
for k in range(num_bootstraps):

    bootstrapSample = resample(dbTraining, n_samples=len(dbTraining), replace=True)

    # populate the values of X_training and y_training by using the bootstrapSample
    # --> add your Python code here
    for sample in bootstrapSample:
        X_training.append(sample[:-1])
        Y_training.append(sample[-1])

    # fitting the decision tree to the data
    # we will use a single decision tree without pruning it
    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=None)
    clf = clf.fit(X_training, Y_training)

    for i, testSample in enumerate(dbTest):

        # make the classifier prediction for each test sample and update the corresponding index value in classVotes. For instance,
        # if your first base classifier predicted 2 for the first test sample, then classVotes[0,0,0,0,0,0,0,0,0,0] will change to classVotes[0,0,1,0,0,0,0,0,0,0].
        # Later, if your second base classifier predicted 3 for the first test sample, then classVotes[0,0,1,0,0,0,0,0,0,0] will change to classVotes[0,0,1,1,0,0,0,0,0,0]
        # Later, if your third base classifier predicted 3 for the first test sample, then classVotes[0,0,1,1,0,0,0,0,0,0] will change to classVotes[0,0,1,2,0,0,0,0,0,0]
        # this array will consolidate the votes of all classifier for all test samples
        # --> add your Python code here
        class_predicted = clf.predict([testSample[:-1]])
        classVotes[i][int(class_predicted)] += 1

        # for only the first base classifier,
        # compare the prediction with the true label of the test sample here to start calculating its accuracy
        if k == 0 and class_predicted == testSample[-1]:
            # --> add your Python code here
            single_correct += 1

    if k == 0:  # for only the first base classifier, print its accuracy here
        # --> add your Python code here
        print("Finished my base classifier (fast but relatively low accuracy) ...")
        print("My base classifier accuracy: " + str(single_correct/len(dbTest)))
        print()

# now, compare the final ensemble prediction (majority vote in classVotes) for each test sample with the ground truth label to calculate the accuracy of the ensemble classifier (all base classifiers together)
# --> add your Python code here
for i, testSample in enumerate(dbTest):
    # check test class against index of max value in votes array
    if classVotes[i].index(max(classVotes[i])) == int(testSample[-1]):
        ensemble_correct += 1

# printing the ensemble accuracy here
print("Finished my ensemble classifier (slow but higher accuracy) ...")
print("My ensemble accuracy: " + str(ensemble_correct/len(dbTest)))
print()

print("Started Random Forest algorithm ...")

# Create a Random Forest Classifier
clf = RandomForestClassifier(n_estimators=20)  # this is the number of decision trees that will be generated by Random Forest. The sample of the ensemble method used before

# Fit Random Forest to the training data
clf.fit(X_training, Y_training)

# make the Random Forest prediction for each test sample. Example: class_predicted_rf = clf.predict([[3, 1, 2, 1, ...]]
# compare the Random Forest prediction for each test sample with the ground truth label to calculate its accuracy
# --> add your Python code here
for i, testSample in enumerate(dbTest):
    if clf.predict([testSample[:-1]]) == testSample[-1]:
        rf_correct += 1

# printing Random Forest accuracy here
print("Random Forest accuracy: " + str(rf_correct/len(dbTest)))

print("Finished Random Forest algorithm (much faster and higher accuracy!) ...")
